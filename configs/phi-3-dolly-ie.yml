base_model: microsoft/Phi-3-mini-128k-instruct
trust_remote_code: true
model_type: AutoModelForCausalLM
tokenizer_type: AutoTokenizer
chat_template: phi_3

load_in_8bit: false
load_in_4bit: false
strict: false

datasets:
  - path: roborovski/dolly-entity-extraction
    type: chat_template
    chat_template: phi_3
    message_preprocessor: entity_extraction
    message_field_role: role
    message_field_content: content

dataset_prepared_path: ./prepared-datasets/dolly_entity_extraction
val_set_size: 16
output_dir: ./lora-out/dolly_entity_extraction
shuffle_before_split: true

sequence_len: 4096
sample_packing: false
pad_to_sequence_len: false

gradient_accumulation_steps: 4
micro_batch_size: 2
num_epochs: 10
optimizer: adamw_torch
adam_beta2: 0.95
adam_epsilon: 0.00001
max_grad_norm: 1.0
lr_scheduler: cosine
learning_rate: 1e-5

bf16: auto
fp16:
tf32: true

train_on_inputs: false
group_by_length: false
save_on_end: false

seed: 1010101

gradient_checkpointing: true
gradient_checkpointing_kwargs:
  use_reentrant: True
logging_steps: 1
flash_attention: true

eval_steps: 10000
save_steps: 10000
eval_table_size: 4
eval_batch_size: 2
eval_sample_packing: false
eval_max_new_tokens: 512
eval_causal_lm_metrics: ["perplexity"]
do_causal_lm_eval: true

warmup_ratio: 0.1
weight_decay: 0.1
resize_token_embeddings_to_32x: true
special_tokens:
  pad_token: "<unk>"
