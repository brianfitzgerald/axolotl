base_model: google/gemma-2-2b-it
trust_remote_code: true
model_type: AutoModelForCausalLM
tokenizer_type: AutoTokenizer
chat_template: gemma
disable_save_on_terminate: true
wandb_name: weave_dev_run
weave_log_eval: true

datasets:
  - path: roborovski/open-goody2
    type: oasst

dataset_prepared_path: ./prepared-datasets/goody
val_set_size: 4
output_dir: ./out
shuffle_before_split: true

sequence_len: 2048
sample_packing: false
pad_to_sequence_len: false

micro_batch_size: 32
gradient_accumulation_steps: 1
num_epochs: 50
optimizer: paged_adamw_32bit
adam_beta2: 0.95
adam_epsilon: 0.00001
max_grad_norm: 1.0
lr_scheduler: linear
learning_rate: 5e-6

fp16: true

seed: 1010101

gradient_checkpointing: true
gradient_checkpointing_kwargs:
  use_reentrant: True
logging_steps: 1
flash_attention: true

eval_steps: 100
save_steps: 5000
eval_table_size: 4
eval_batch_size: 4
eval_sample_packing: false
eval_max_new_tokens: 512
eval_causal_lm_metrics: ["perplexity", "ter"]
do_causal_lm_eval: true

warmup_ratio: 0.2
weight_decay: 0.1
resize_token_embeddings_to_32x: true
